{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsDv26CsstHl"
      },
      "source": [
        "# Reinforcement Learning: Frozen Lake Problem\n",
        "\n",
        "In this notebook we will learn about reinforcement learning using [Open AI Gym's Frozen Lake environment](https://gym.openai.com/envs/FrozenLake-v0/). Read over the description in the link so that you understand the environment.\n",
        "\n",
        "Specifically, we will use a technique called **Value Iteration** to solve the Markov Decision Process created by Open AI Gym. With this technique we build up our agent's policy by repeatedly looping through all possible states and determining how \"good\" any given state is, or how much *value* it adds towards reaching our goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZQOh0axuNYC"
      },
      "source": [
        "We start by importing the necessary modules and creating the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDvHmLydCCBf",
        "outputId": "769c5deb-0c1c-41fb-ab2d-b2a093df6719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.17.3 in /usr/local/lib/python3.9/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from gym==0.17.3) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.17.3) (1.6.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from gym==0.17.3) (1.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n",
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gym==0.17.3\n",
        "import gym\n",
        "import time\n",
        "import statistics\n",
        "import random\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#action_space.sample() provides us a random action\n",
        "env.action_space.sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWbkEmWNHhWU",
        "outputId": "f2ee14b0-bc09-4869-ef18-47ee2f4ad2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reset() puts back the environment to its original state\n",
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0DADrVDHpsZ",
        "outputId": "d3770c89-e0a3-4fdc-bdf8-ff255faaaaa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step() allows us to make the agent do something. We pass an object and it returns us a tuple (observation, reward, done or not done, info)\n",
        "env.step(3)\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak5s4N5mH633",
        "outputId": "d63b8960-d67b-4074-bc3f-942f861826bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Up)\n",
            "SFFFFFF\u001b[41mF\u001b[0m\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "#Challenge problem (my solution)\n",
        "#random.randrange() is horrible for probability stuff like this\n",
        "#OpenAI already made something for better use\n",
        "\"\"\"\n",
        "actions = []\n",
        "for i in range(1000):\n",
        "  actions.append(random.randrange(0,4))\n",
        "  env.step(actions[i])\n",
        "\"\"\"\n",
        "\n",
        "for _ in range(1000):\n",
        "  env.step(env.action_space.sample)\n",
        "\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yA9pRXKJadp",
        "outputId": "27bdc73c-230e-4ffd-e958-ab3632e961a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFF\u001b[41mH\u001b[0mFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly_4xiFpS-6F",
        "outputId": "5ac7f36b-f390-476c-f927-42d0781d38e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTtOGGSeuqDm"
      },
      "source": [
        "Here we will define a function to determine the value or \"goodness\" of any given action.\n",
        "\n",
        "The function works as follows:\n",
        "**We loop through all possible states that could follow from the given action,**\n",
        " these possibile states are given by `possibilities`. The environment gives us the probability of going to that state as well as the reward we receive for moving to that state.\n",
        "\n",
        "The formula for the goodness of an action from the current state takes into account the reward for moving to that state, as well how \"good\" we had previously determined this state was, then it factors in the probability of actually moving to this particular state. It does this for all possible states and sums them up to figure out how \"good\" this action is.\n",
        "\n",
        "The `gamma` variable here determines how much our previous determinations\n",
        "of the next state's goodness factors into our decision.\n",
        "\n",
        "**`env.P[state][action]` <mark>here serves to provide us the values of** </mark> $P_a(s, s')$ and $R_a(s, s')$ from the MDP for every possible next state $s'$ from our current state (`state`) given `action`. Specifically, it gives <mark>**a sequence of tuples containing the probability ($P_a(s, s')$), the next state ($s'$), and the reward ($R_a(s, s')$).**</mark>\n",
        "\n",
        "This function corresponds to the\n",
        "\n",
        "$$\n",
        "\\sum_{s'} P_a(s', s)(R_a(s,s') + \\gamma V_i(s'))\n",
        "$$\n",
        "\n",
        "portion of value iteration formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUP52nkQaZEb"
      },
      "outputs": [],
      "source": [
        "# Note: this can be added in later once it is needed, might make more sense then\n",
        "def calc_action_value(state: int, action: int, state_func: list,\n",
        "                      gamma: float) -> float:\n",
        "    action_value = 0\n",
        "    # env.P[state][action] gives us the possible next states from\n",
        "    # `state` by taking `action`\n",
        "    for prob, next_state, reward, _ in env.P[state][action]:\n",
        "        action_value += prob * (reward + gamma * state_func[next_state])\n",
        "\n",
        "    return action_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D32_AvBqw2j-"
      },
      "source": [
        "Next we implement the <mark>Value Iteration algorithm. The purpose of this algorithm is to assign a value to each state.</mark> The greater the value of a state, the better that state is for accomplishing our goal. We accomplish this by repeatedly looping through all the states in our environment, then computing the value of that state based on the rewards available to us from that state.\n",
        "\n",
        "On repeated iterations through the states, the previously computed value of each state is also factored into a state's value calculation. For example, if we determine that one state has a high value because it is right next to the goal, then these repeated iterations would determine that states around this state also have a high value because the goal can be reached from them.\n",
        "\n",
        "Here is the algorithm for value iteration again:\n",
        "\n",
        "Loop until the left hand side is approximately equal to the right hand side:\n",
        "\n",
        "$$\n",
        "V_{i+1}(s) := \\max_a \\bigg\\{ \\sum_{s'} P_a(s', s)(R_a(s,s') + \\gamma V_i(s')) \\bigg\\}\n",
        "$$\n",
        "\n",
        "**where $i$ is the iteration number, and $s$ is the current state which we are\n",
        "calculating the goodness of.**\n",
        "\n",
        "\n",
        "**In our code, `state_func` is the value of $V_i$, `new_state_func` is the value of $V_{i+1}$, `state` is $s$, `action` is $a$, `i` is $i$, and `gamma` is $\\gamma$.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16gUzMcOB665",
        "outputId": "f77368fb-003c-499c-ca8e-051d2e528158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.006411114261567677, 0.008548152348756905, 0.012300498232638183, 0.017789476934689102, 0.02508218296759626, 0.03247093431161935, 0.03957138135942687, 0.0429784849399046, 0.0060241306468695474, 0.007645190581127934, 0.010911685608133328, 0.01642659654667831, 0.026054159279438605, 0.03619413203435161, 0.04935473823209144, 0.05730464658653949, 0.005090317113302161, 0.005853275950835495, 0.006775412141688225, 0, 0.025570882596180823, 0.03882143423746126, 0.0676397661610051, 0.08435610380316738, 0.004225683724205108, 0.00476961157650976, 0.005819745579991928, 0.007854128215108443, 0.02036068180370288, 0, 0.09175504516809152, 0.12919114271305215, 0.0031810051723101546, 0.0031966616778519204, 0.0027049221773531545, 0, 0.034443928534387, 0.06195147262370438, 0.10901924168624784, 0.2096909544956969, 0.0018692504620791625, 0, 0, 0.010850801897953477, 0.032500940687216075, 0.06304173852504644, 0, 0.3600877511102408, 0.0011805792392078917, 0, 0.0013771946774760297, 0.003668398972628852, 0, 0.115686715105901, 0, 0.630513798094865, 0.0008854344294059186, 0.0007747218778702138, 0.0009222499522912471, 0, 0.13824884792626724, 0.32258064516129026, 0.6144393241167434, 0]\n",
            "[3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 1, 3, 3, 0, 0, 2, 3, 2, 1, 3, 3, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 2, 1, 3, 2, 0, 0, 0, 1, 3, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 1, 0]\n",
            "Size of state_func 64\n",
            "Size of policy 64\n"
          ]
        }
      ],
      "source": [
        "def value_iteration(env, max_iterations=100000, gamma=0.9):\n",
        "    \"\"\"Determines how \"good\" any given state is to be in for our actor.\n",
        "\n",
        "    Returns: an array with a value for each state, and another array for the policy.\n",
        "        The greater a given value in the first array, the \"better\" the state.\n",
        "    \"\"\"\n",
        "    # An array where each item represents how \"good\" a state is to be in\n",
        "    state_func = [0] * env.nS\n",
        "    # We'll update the state function gradually and use this copy to do it\n",
        "    new_state_func = state_func.copy()\n",
        "    # Our policy will contain the best action for any given state\n",
        "    # Do this after the initial value iteration\n",
        "    policy = [0] * env.nS\n",
        "\n",
        "    # Prevent looping infinitely if our algorithm doesn't converge\n",
        "    for i in range(max_iterations):\n",
        "        # Loop through all possible states\n",
        "        #(n is the *S*tates. the *n*umber of *S*tates)\n",
        "        for state in range(env.nS):\n",
        "            # For each state we find the best possible action to take in that state\n",
        "            best_state_action_val = 0\n",
        "            # Do this after the initial value iteration\n",
        "            best_action = 0\n",
        "            # So we try all the actions\n",
        "            for action in range(env.nA):\n",
        "                state_action_val = calc_action_value(state, action, state_func, gamma)\n",
        "\n",
        "                # After calculating the goodness of this action, we keep it only if it is\n",
        "                # better than the previous best\n",
        "                if state_action_val > best_state_action_val:\n",
        "                    best_state_action_val = state_action_val\n",
        "                    # Do this after the initial value iteration\n",
        "                    best_action = action\n",
        "\n",
        "            # After calculating the best possible action for this state, we save how\n",
        "            # \"good\" the best action is for the state...\n",
        "            new_state_func[state] = best_state_action_val\n",
        "            # And we remember the action for later\n",
        "            # Do this after the initial value iteration\n",
        "            policy[state] = best_action\n",
        "\n",
        "        # After 1000 iterations, if the state function hasn't improved very much\n",
        "        # we stop trying to improve it\n",
        "        if i > 1000 and sum(state_func) - sum(new_state_func) < 1e-04:\n",
        "            break\n",
        "\n",
        "        # Otherwise we update the state function to the newly improved version\n",
        "        state_func = new_state_func.copy()\n",
        "\n",
        "    # After figuring out the goodness of each state and the best actions we return them\n",
        "    return state_func, policy\n",
        "\n",
        "state_func, policy = value_iteration(env)\n",
        "print(state_func)\n",
        "print(policy)\n",
        "print(\"Size of state_func\", len(state_func))\n",
        "print(\"Size of policy\", len(policy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGrs-b6HWnk"
      },
      "source": [
        "Now that we have the value or \"goodness\" of any given state, we need to use these values to build a *policy*. The policy defines the action that should be taken from any given state. This action should be based on the value of the states around the given state.\n",
        "\n",
        "For example, if we are in state `A` and moving right means we are likely to move to state `B` which a value of 2, and moving left means moving to state `C` which has a value of 1, our policy should tell us that we should take the \"right\" action instead of the \"left\".\n",
        "\n",
        "Let's go through our value iteration function above now and update it to remember the best action for each state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH2gxXW2K12-"
      },
      "source": [
        "Now our agent is ready to navigate its way through the frozen lake! It has a policy which will tell it the best action to take in any of the 64 states, so all we have to do is tell it to act based on the policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdQXbwBJBVX0",
        "outputId": "4e829078-6049-41c7-8d9f-84b6e0d7e9b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------\n",
            "You took an average of 73 steps to get the frisbee\n",
            "And you fell in the hole 24.70% of the time\n",
            "----------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def get_score(env, policy, episodes=1000):\n",
        "    misses = 0\n",
        "    steps_list = []\n",
        "    best_episode = []\n",
        "    # We try to navigate the lake `episodes` number of times\n",
        "    # \"In other words, we're 'playing' this game episode number of times\"\n",
        "    for _ in range(episodes):\n",
        "        # We reset the environment so we're back at the start, and store the current\n",
        "        # state in `observation`\n",
        "        observation = env.reset()\n",
        "        episode = []\n",
        "        steps = 0\n",
        "        while True:\n",
        "            # We use our policy to determine the best action based on the current state\n",
        "            #\"Given that I'm observing, what should I do?\"\n",
        "            action = policy[observation]\n",
        "            # We tell the agent to take the action and we retrieve the new state,\n",
        "            # the reward for moving to that state, and also if we are done or not\n",
        "            observation, reward, done, _ = env.step(action)\n",
        "            # We'll save a string representation of the environment so we can watch\n",
        "            # it later\n",
        "            episode.append(env.render(mode='ansi'))\n",
        "            steps += 1\n",
        "            # If we finished and reached the goal\n",
        "            if done and reward == 1:\n",
        "                steps_list.append(steps)\n",
        "\n",
        "                # We save this episode if we reached the goal in fewer steps\n",
        "                if len(best_episode) == 0 \\\n",
        "                    or len(episode) < len(best_episode):\n",
        "                    best_episode = episode\n",
        "\n",
        "                break\n",
        "            # If we finished but fell in a hole\n",
        "            elif done and reward == 0:\n",
        "                misses += 1\n",
        "                break\n",
        "\n",
        "    print('----------------------------------------------')\n",
        "    print('You took an average of {:.0f} steps to get the frisbee'.format(\n",
        "        statistics.mean(steps_list)))\n",
        "    print('And you fell in the hole {:.2f}% of the time'.format(\n",
        "        (misses / episodes) * 100))\n",
        "    print('----------------------------------------------')\n",
        "\n",
        "    return best_episode\n",
        "\n",
        "\n",
        "best_episode = get_score(env, policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5W7Bq7ZolYn",
        "outputId": "f9a32bda-7b3f-4191-a260-778dd5980b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import output\n",
        "\n",
        "for e in best_episode:\n",
        "    output.clear()\n",
        "    print(e)\n",
        "    time.sleep(0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Chf_9g_QS1l"
      },
      "source": [
        "**Challenges**:\n",
        "\n",
        "- What's the best score you can get?\n",
        "- To test your understanding, update `value_iteration` so that it no longer builds the policy while finding the values, and create a new function `get_policy` that builds the policy based on the values returned from `value_iteration`.\n",
        "    - Hint: for every state, you will need to find the action which gives you the maximum `calc_action_value`, and save that action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBpVseNKqIqP"
      },
      "source": [
        "## Sources\n",
        "\n",
        "- Based on [this article](https://medium.com/analytics-vidhya/solving-the-frozenlake-environment-from-openai-gym-using-value-iteration-5a078dffe438)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}